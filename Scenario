# src/train.py

import pandas as pd
import joblib
from extract import load_csv, save_pickle
from labelers import (
    label_anomaly,
    label_card_share,
    label_duration,
    label_low_swipe,
    label_exit_first,
    label_simultaneous,
)
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# --------------------------------------------------------------------------------
# 1. Apply all labelers in sequence to produce a “labeled” DataFrame
# --------------------------------------------------------------------------------
def apply_all_labelers(df: pd.DataFrame) -> pd.DataFrame:
    # 1) Anomaly labeling
    df = label_anomaly(df, window_minutes=10, max_swipes=5)

    # 2) Card‐share labeling
    df = label_card_share(df, window=pd.Timedelta(minutes=5))

    # 3) Duration labeling: returns a separate DataFrame (one row per EmployeeID+Date)
    df_duration = label_duration(df)

    # 4) Low‐swipe labeling (one & two swipes): also returns separate DataFrame
    df_low_swipe = label_low_swipe(df)

    # 5) Exit‐first labeling
    df_exit_first = label_exit_first(df)

    # 6) Simultaneous‐swipe labeling
    df = label_simultaneous(df)

    # We’ll now merge all “per‐day” labels (duration, low_swipe, exit_first) back onto the main swipe‐level df.
    # Merge duration info:
    df = df.merge(
        df_duration[["EmployeeID", "Dateonly", "duration_hours", "short_stay", "long_stay"]],
        on=["EmployeeID", "Dateonly"],
        how="left",
    )
    # Merge low‐swipe info:
    df = df.merge(
        df_low_swipe[["EmployeeID", "Dateonly", "swipe_count", "one_swipe", "two_swipe"]],
        on=["EmployeeID", "Dateonly"],
        how="left",
    )
    # Merge exit‐first info:
    df = df.merge(
        df_exit_first[["EmployeeID", "Dateonly", "exit_first"]],
        on=["EmployeeID", "Dateonly"],
        how="left",
    )

    return df


# --------------------------------------------------------------------------------
# 2. Engineer “features” from the labeled DataFrame
# --------------------------------------------------------------------------------
def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    # ====================================================
    # If “zone” isn’t present at all, create a default column
    # ====================================================
    if "zone" not in df.columns:
        df["zone"] = "Unknown"
    else:
        df["zone"] = df["zone"].fillna("Unknown")

    # Example of filling other columns that might have NaNs:
    df["EmployeeID"] = df["EmployeeID"].fillna("UnknownID")
    df["PersonnelType"] = df["PersonnelType"].fillna("UnknownType")
    df["Door"] = df["Door"].fillna("UnknownDoor")

    # Convert any categorical columns to category dtype (optional, improves memory)
    df["PersonnelType"] = df["PersonnelType"].astype("category")
    df["zone"] = df["zone"].astype("category")
    df["Door"] = df["Door"].astype("category")

    # Extract hour of swipe as a numeric feature
    df["hour"] = df["LocaleMessageTime"].dt.hour

    # Encode “InDirection” vs “OutDirection”
    df["is_in"] = (df["Direction"] == "InDirection").astype(int)

    # Label columns like 'anomaly', 'card_share', 'simultaneous', 'one_swipe', 'two_swipe', 'exit_first'
    # already exist if you ran apply_all_labelers. If any are still missing, fill with False.
    for col in ["anomaly", "card_share", "simultaneous", "one_swipe", "two_swipe", "exit_first", "short_stay", "long_stay"]:
        if col not in df.columns:
            df[col] = False
        else:
            df[col] = df[col].fillna(False)

    # For simplicity, let's treat PersonType and zone by one‐hot encoding
    df = pd.get_dummies(df, columns=["PersonnelType", "zone", "Door"], drop_first=True)

    # Example: we’ll build a binary target “is_anomaly” from the anomaly column (just for demo)
    df["is_anomaly"] = df["anomaly"].astype(int)

    # Select a small set of numeric features and the target
    # (you can expand this list as desired)
    feature_cols = [
        "hour",
        "is_in",
        "duration_hours",
        "swipe_count",
        "one_swipe",
        "two_swipe",
        "exit_first",
        "simultaneous",
        "card_share",
        "short_stay",
        "long_stay",
    ]
    # Add all generated dummy columns
    dummy_cols = [c for c in df.columns if c.startswith("PersonnelType_") or c.startswith("zone_") or c.startswith("Door_")]
    feature_cols.extend(dummy_cols)

    # Drop rows where our chosen features contain NaN (just to be safe)
    df_features = df[feature_cols + ["is_anomaly"]].dropna()

    return df_features


# --------------------------------------------------------------------------------
# 3. Train a simple RandomForest model (for demonstration)
# --------------------------------------------------------------------------------
def build_and_save_model():
    # a) Load the CSV‐derived DataFrame (or pickle if you prefer)
    df = load_csv()  # assumes extract.py has already created data/logs.pkl
    df_labeled = apply_all_labelers(df)

    # b) Engineer features (this step no longer crashes on missing “zone”)
    df_feat = engineer_features(df_labeled)

    # c) Split into train/test
    X = df_feat.drop(columns=["is_anomaly"])
    y = df_feat["is_anomaly"]
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # d) Train a RandomForest (example)
    clf = RandomForestClassifier(n_estimators=50, random_state=42)
    clf.fit(X_train, y_train)

    # e) Evaluate on test set
    preds = clf.predict(X_test)
    print("\n=== Classification Report ===")
    print(classification_report(y_test, preds, zero_division=0))

    # f) Save the trained model to disk
    joblib.dump(clf, "models/rf_anomaly_model.joblib")
    print("✅ Model saved to models/rf_anomaly_model.joblib")


if __name__ == "__main__":
    build_and_save_model()











# src/labelers.py

import pandas as pd
from datetime import timedelta

def label_anomaly(df: pd.DataFrame, window_minutes: int = 10, max_swipes: int = 5) -> pd.DataFrame:
    """
    Label 'anomaly' if an employee has more than `max_swipes` within `window_minutes`.
    This version avoids duplicate‐label reindexing by computing each employee's rolling counts
    and assigning them back to the original DataFrame row by row.
    """
    # Work on a copy so we don’t mutate the caller’s DataFrame
    df = df.copy()

    # Ensure LocaleMessageTime is datetime and drop rows with invalid timestamps
    df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    df = df.dropna(subset=["LocaleMessageTime"])

    # Sort by EmployeeID then timestamp
    df = df.sort_values(["EmployeeID", "LocaleMessageTime"])

    # Create a new column to hold the rolling‐window counts (initialized to zero)
    df["swipe_count_rolling"] = 0

    # For each distinct EmployeeID, compute the rolling count of swipes in the given time window
    # and write those counts back into df["swipe_count_rolling"] at the correct row indices.
    for emp_id, index_list in df.groupby("EmployeeID").groups.items():
        # Extract this employee’s rows (preserving original index labels)
        sub = df.loc[index_list].copy()
        sub["orig_index"] = sub.index

        # Temporarily set the timestamp column as the index so we can call .rolling("Xmin")
        sub2 = sub.set_index("LocaleMessageTime")

        # Now perform a time‐based rolling count (count how many rows fall in each window)
        rolling_counts = sub2["EmployeeID"].rolling(f"{window_minutes}min").count()

        # rolling_counts.values is an array aligned in the same order as sub2’s rows—
        # we also kept sub2["orig_index"] so we can map each count back to df’s row index.
        counts_array = rolling_counts.values
        orig_indices = sub2["orig_index"].values

        # Assign each rolling count back to the correct row in the original df
        for orig_idx, count_val in zip(orig_indices, counts_array):
            df.at[orig_idx, "swipe_count_rolling"] = int(count_val)

    # Mark anomaly if swipe_count_rolling > max_swipes
    df["anomaly"] = df["swipe_count_rolling"] > max_swipes

    return df


def label_card_share(df: pd.DataFrame, window: timedelta = timedelta(minutes=5)) -> pd.DataFrame:
    """
    Label 'card_share' when two different EmployeeIDs use the same CardNumber within `window`.
    """
    df = df.copy()
    df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    df = df.sort_values("LocaleMessageTime")

    # For each CardNumber, examine the previous swipe’s EmployeeID and timestamp
    df["prev_employee"] = df.groupby("CardNumber")["EmployeeID"].shift(1)
    df["prev_time"]     = df.groupby("CardNumber")["LocaleMessageTime"].shift(1)

    # Mark card_share = True if (1) EmployeeID changed and (2) time difference ≤ window
    df["card_share"] = (
        (df["EmployeeID"] != df["prev_employee"]) &
        (df["LocaleMessageTime"] - df["prev_time"] <= window)
    ).fillna(False)

    return df


def label_duration(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute duration per day and label 'short_stay' (<1h) and 'long_stay' (>10h).
    Returns a DataFrame grouped by (EmployeeID, Dateonly) with:
      first_swipe, last_swipe, swipe_count, duration_hours, short_stay, long_stay
    """
    df = df.copy()
    df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    # Aggregate per EmployeeID & Dateonly
    agg = (
        df.groupby(["EmployeeID", "Dateonly"])["LocaleMessageTime"]
          .agg(["min", "max", "count"])
          .rename(columns={"min": "first_swipe", "max": "last_swipe", "count": "swipe_count"})
          .reset_index()
    )

    # Compute duration in hours, then label as short_stay or long_stay
    agg["duration_hours"] = (agg["last_swipe"] - agg["first_swipe"]).dt.total_seconds() / 3600
    agg["short_stay"] = agg["duration_hours"] < 1
    agg["long_stay"]  = agg["duration_hours"] > 10

    return agg


def label_low_swipe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Label 'one_swipe' or 'two_swipe' if total swipes per day ≤ 2.
    Returns a DataFrame with (EmployeeID, Dateonly, swipe_count, one_swipe, two_swipe).
    """
    df = df.copy()
    counts = (
        df.groupby(["EmployeeID", "Dateonly"])
          .size()
          .reset_index(name="swipe_count")
    )
    counts["one_swipe"] = counts["swipe_count"] == 1
    counts["two_swipe"] = counts["swipe_count"] == 2

    return counts


def label_exit_first(df: pd.DataFrame) -> pd.DataFrame:
    """
    Label 'exit_first' if the first swipe of the day is OutDirection.
    Returns a DataFrame with (EmployeeID, Dateonly, exit_first).
    """
    df = df.copy()
    df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    first = (
        df.sort_values("LocaleMessageTime")
          .groupby(["EmployeeID", "Dateonly"])
          .first()
          .reset_index()
    )
    first["exit_first"] = first["Direction"] == "OutDirection"

    return first[["EmployeeID", "Dateonly", "exit_first"]]


def label_simultaneous(df: pd.DataFrame) -> pd.DataFrame:
    """
    Label 'simultaneous' when two different EmployeeIDs swipe the same Door at exactly the same time.
    Returns the original DataFrame merged with a new 'simultaneous' column (True/False).
    """
    df = df.copy()
    df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    dup = (
        df.groupby(["Door", "LocaleMessageTime"])["EmployeeID"]
          .nunique()
          .reset_index(name="user_count")
    )
    dup["simultaneous"] = dup["user_count"] > 1

    df = df.merge(
        dup[["Door", "LocaleMessageTime", "simultaneous"]],
        on=["Door", "LocaleMessageTime"],
        how="left"
    )
    df["simultaneous"] = df["simultaneous"].fillna(False)

    return df









PS C:\Users\W0024618\Desktop\ai-employee-security\src> python train.py
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\ai-employee-security\src\train.py", line 188, in <module>
    build_and_save_model()
    ~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\ai-employee-security\src\train.py", line 124, in build_and_save_model
    df_labeled = apply_all_labelers(df)
  File "C:\Users\W0024618\Desktop\ai-employee-security\src\train.py", line 39, in apply_all_labelers
    df = label_anomaly(df)
  File "C:\Users\W0024618\Desktop\ai-employee-security\src\labelers.py", line 25, in label_anomaly
    df["swipe_count_rolling"] = (
    ~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\ai-employee-security\venv\Lib\site-packages\pandas\core\frame.py", line 4311, in __setitem__
    self._set_item(key, value)
    ~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\ai-employee-security\venv\Lib\site-packages\pandas\core\frame.py", line 4524, in _set_item
    value, refs = self._sanitize_column(value)
                  ~~~~~~~~~~~~~~~~~~~~~^^^^^^^
  File "C:\Users\W0024618\Desktop\ai-employee-security\venv\Lib\site-packages\pandas\core\frame.py", line 5263, in _sanitize_column
    return _reindex_for_setitem(value, self.index)
  File "C:\Users\W0024618\Desktop\ai-employee-security\venv\Lib\site-packages\pandas\core\frame.py", line 12692, in _reindex_for_setitem
    raise err
  File "C:\Users\W0024618\Desktop\ai-employee-security\venv\Lib\site-packages\pandas\core\frame.py", line 12687, in _reindex_for_setitem
    reindexed_value = value.reindex(index)._values
                      ~~~~~~~~~~~~~^^^^^^^
  File "C:\Users\W0024618\Desktop\ai-employee-security\venv\Lib\site-packages\pandas\core\series.py", line 5153, in reindex
    return super().reindex(
           ~~~~~~~~~~~~~~~^
        index=index,
        ^^^^^^^^^^^^
    ...<5 lines>...
        tolerance=tolerance,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\W0024618\Desktop\ai-employee-security\venv\Lib\site-packages\pandas\core\generic.py", line 5610, in reindex
    return self._reindex_axes(
           ~~~~~~~~~~~~~~~~~~^
        axes, level, limit, tolerance, method, fill_value, copy
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ).__finalize__(self, method="reindex")
    ^
  File "C:\Users\W0024618\Desktop\ai-employee-security\venv\Lib\site-packages\pandas\core\generic.py", line 5633, in _reindex_axes
    new_index, indexer = ax.reindex(
                         ~~~~~~~~~~^
        labels, level=level, limit=limit, tolerance=tolerance, method=method
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\W0024618\Desktop\ai-employee-security\venv\Lib\site-packages\pandas\core\indexes\base.py", line 4429, in reindex
    raise ValueError("cannot reindex on an axis with duplicate labels")
ValueError: cannot reindex on an axis with duplicate labels
PS C:\Users\W0024618\Desktop\ai-employee-security\src> 
