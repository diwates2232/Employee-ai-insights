# src/train.py
import os
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import joblib

# Project paths
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
DATA_DIR     = os.path.join(PROJECT_ROOT, "data")
MODEL_DIR    = os.path.join(PROJECT_ROOT, "models")
PICKLE_NAME  = "logs.pkl"

# Make sure the models/ folder exists
os.makedirs(MODEL_DIR, exist_ok=True)

# 1) Load logs.pkl
def load_pickled_logs(filename: str = PICKLE_NAME) -> pd.DataFrame:
    path = os.path.join(DATA_DIR, filename)
    if not os.path.exists(path):
        raise FileNotFoundError(f"No such file: {path}")
    df = pd.read_pickle(path)
    return df

# 2) Import labelers and apply scenario labels
from labelers import (
    label_anomaly,
    label_card_share,
    label_duration,
    label_low_swipe,
    label_exit_first,
    label_simultaneous,
)

def apply_all_labelers(df: pd.DataFrame) -> pd.DataFrame:
    df = label_anomaly(df)
    df = label_card_share(df)
    # label_duration returns an aggregated DF; we merge on EmployeeID+Dateonly
    duration_df = label_duration(df)
    df = df.merge(
        duration_df[["EmployeeID", "Dateonly", "duration_hours", "short_stay", "long_stay"]],
        on=["EmployeeID", "Dateonly"],
        how="left",
    )
    low_swipe_df = label_low_swipe(df)
    df = df.merge(
        low_swipe_df[["EmployeeID", "Dateonly", "one_swipe", "two_swipe"]],
        on=["EmployeeID", "Dateonly"],
        how="left",
    )
    exit_first_df = label_exit_first(df)
    df = df.merge(
        exit_first_df[["EmployeeID", "Dateonly", "exit_first"]],
        on=["EmployeeID", "Dateonly"],
        how="left",
    )
    df = label_simultaneous(df)
    return df

# 3) Feature engineering
def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # Ensure datetime
    df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"])

    # Basic time features
    df["hour"] = df["LocaleMessageTime"].dt.hour
    df["minute"] = df["LocaleMessageTime"].dt.minute
    df["day_of_week"] = df["LocaleMessageTime"].dt.dayofweek  # Monday=0..Sunday=6
    df["is_weekend"] = df["day_of_week"].isin([5,6]).astype(int)

    # Duration columns already merged by label_duration
    # short_stay, long_stay, one_swipe, two_swipe, exit_first, simultaneous, anomaly, card_share

    # Zone (categorical)
    df["zone"] = df["zone"].fillna("Unknown")
    
    # Door (categorical)
    df["Door"] = df["Door"].fillna("Unknown")
    
    # PersonnelType (categorical)
    df["PersonnelType"] = df["PersonnelType"].fillna("Unknown")

    # Combine EmployeeID as string (might have alphanumeric)
    df["EmployeeID"] = df["EmployeeID"].astype(str)

    # We’ll select a subset of features for the model:
    feature_cols = [
        "hour",
        "day_of_week",
        "is_weekend",
        # scenario labels:
        "short_stay",
        "long_stay",
        "one_swipe",
        "two_swipe",
        "exit_first",
        "simultaneous",
        "anomaly",
        "card_share",
        # categorical:
        "zone",
        "Door",
        "PersonnelType",
    ]

    # Convert bools to 0/1:
    for col in ["short_stay","long_stay","one_swipe","two_swipe","exit_first","simultaneous","anomaly","card_share"]:
        df[col] = df[col].astype(int)

    # Keep only feature columns
    X = df[feature_cols].copy()
    return X

def build_and_save_model():
    # Load raw DataFrame
    df = load_pickled_logs()

    # Apply all labelers (adds columns: anomaly, card_share, etc.)
    df_labeled = apply_all_labelers(df)

    # Engineer numeric + categorical features
    X = engineer_features(df_labeled)

    # We train an IsolationForest on the *entire* dataset (unsupervised).
    # Normally you might want to exclude rows that you already know are "anomalies",
    # but here we assume the majority is "normal" and you just want outlier detection.
    # 
    # In an IsolationForest, by default `predict(X)` returns +1 for inliers, -1 for outliers.
    #
    # Column types:
    #   - numeric: hour, day_of_week, is_weekend, short_stay, long_stay, one_swipe, two_swipe, exit_first, simultaneous, anomaly, card_share
    #   - categorical: zone, Door, PersonnelType (will one‐hot encode)

    numeric_features = [
        "hour",
        "day_of_week",
        "is_weekend",
        "short_stay",
        "long_stay",
        "one_swipe",
        "two_swipe",
        "exit_first",
        "simultaneous",
        "anomaly",
        "card_share",
    ]
    categorical_features = ["zone", "Door", "PersonnelType"]

    # Build a simple preprocessing pipeline:
    numeric_transformer = "passthrough"  # already numeric
    categorical_transformer = OneHotEncoder(handle_unknown="ignore", sparse=False)

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
    )

    # Full Pipeline: preprocessing → IsolationForest
    clf = Pipeline(
        steps=[
            ("preproc", preprocessor),
            ("clf", IsolationForest(
                n_estimators=100,
                contamination=0.02,  # adjust to expected fraction of anomalies
                random_state=42,
                verbose=1,
            )),
        ]
    )

    print("⏳  Building IsolationForest on {} rows and {} features...".format(X.shape[0], X.shape[1]))
    clf.fit(X)

    # Save the entire pipeline (so it includes preprocessing + model)
    model_path = os.path.join(MODEL_DIR, "isolation_forest_model.joblib")
    joblib.dump(clf, model_path)
    print(f"✅ Model saved to {model_path}")

if __name__ == "__main__":
    build_and_save_model()








# src/api.py

import os
import pandas as pd
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional
import joblib

from myapp.data_controller import router as data_router

app = FastAPI(
    title="AI Employee Security Backend",
    description="Endpoints to fetch historical access‐log data and predict anomalies",
    version="1.0.0",
)

# Include the /raw-data endpoint from your existing data_controller
app.include_router(data_router, prefix="/api")


# -------------- New: Load the trained IsolationForest pipeline --------------
MODEL_PATH = os.path.abspath(
    os.path.join(os.path.dirname(__file__), os.pardir, "models", "isolation_forest_model.joblib")
)
try:
    pipeline = joblib.load(MODEL_PATH)
except Exception as e:
    pipeline = None
    print(f"⚠️  Could not load model at {MODEL_PATH}: {e}")



# -------------- Pydantic model for a single swipe-record --------------
class SwipeRecord(BaseModel):
    LocaleMessageTime: pd.Timestamp = Field(..., description="ISO‐8601 timestamp")
    EmployeeID: str
    ObjectName1: Optional[str] = None
    PersonnelType: Optional[str] = None
    CardNumber: Optional[str] = None
    AdmitCode: Optional[str] = None
    Direction: Optional[str] = None
    Door: Optional[str] = None
    Rejection_Type: Optional[str] = None
    Dateonly: Optional[pd.Timestamp] = None
    Swipe_Time: Optional[pd.Timestamp] = None

    class Config:
        orm_mode = True


# -------------- /api/predict-anomaly endpoint --------------
@app.post("/api/predict-anomaly", response_model=List[dict])
async def predict_anomaly(records: List[SwipeRecord]):
    """
    Accept a JSON array of swipe‐records, run the same feature‐engineering
    and return an anomaly score (−1 = anomaly, +1 = normal) for each record.
    """
    if pipeline is None:
        raise HTTPException(status_code=500, detail="Model not loaded on server.")

    # Convert incoming JSON to a DataFrame
    try:
        df_in = pd.DataFrame([r.dict() for r in records])
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Malformed input: {e}")

    # 1) Ensure datetime parsing
    df_in["LocaleMessageTime"] = pd.to_datetime(df_in["LocaleMessageTime"])
    df_in["Dateonly"]         = df_in["LocaleMessageTime"].dt.date
    df_in["Swipe_Time"]       = df_in["LocaleMessageTime"].dt.time

    # 2) We need to replicate exactly the same feature‐engineering steps that train.py used.
    # Import your labelers and feature‐engineering functions:
    from extract import load_csv  # To pick up column names if needed
    from labelers import (
        label_anomaly,
        label_card_share,
        label_duration,
        label_low_swipe,
        label_exit_first,
        label_simultaneous,
    )
    # Step A: Apply each labeler in the same order:
    temp_df = df_in.copy()
    temp_df = label_anomaly(temp_df)
    temp_df = label_card_share(temp_df)
    duration_df = label_duration(temp_df)
    temp_df = temp_df.merge(
        duration_df[["EmployeeID", "Dateonly", "duration_hours", "short_stay", "long_stay"]],
        on=["EmployeeID", "Dateonly"],
        how="left",
    )
    low_swipe_df = label_low_swipe(temp_df)
    temp_df = temp_df.merge(
        low_swipe_df[["EmployeeID", "Dateonly", "one_swipe", "two_swipe"]],
        on=["EmployeeID", "Dateonly"],
        how="left",
    )
    exit_first_df = label_exit_first(temp_df)
    temp_df = temp_df.merge(
        exit_first_df[["EmployeeID", "Dateonly", "exit_first"]],
        on=["EmployeeID", "Dateonly"],
        how="left",
    )
    temp_df = label_simultaneous(temp_df)

    # 3) Engineer features exactly as in train.py
    def engineer_single(df):
        df = df.copy()
        df["hour"] = df["LocaleMessageTime"].dt.hour
        df["day_of_week"] = df["LocaleMessageTime"].dt.dayofweek
        df["is_weekend"] = df["day_of_week"].isin([5,6]).astype(int)

        # Fill missing booleans with 0
        for col in ["short_stay","long_stay","one_swipe","two_swipe","exit_first","simultaneous","anomaly","card_share"]:
            if col not in df.columns:
                df[col] = 0
            else:
                df[col] = df[col].astype(int)

        # Categorical
        df["zone"] = df.get("zone", pd.Series(["Unknown"]*len(df))).fillna("Unknown")
        df["Door"] = df.get("Door", pd.Series(["Unknown"]*len(df))).fillna("Unknown")
        df["PersonnelType"] = df.get("PersonnelType", pd.Series(["Unknown"]*len(df))).fillna("Unknown")

        feature_cols = [
            "hour",
            "day_of_week",
            "is_weekend",
            "short_stay",
            "long_stay",
            "one_swipe",
            "two_swipe",
            "exit_first",
            "simultaneous",
            "anomaly",
            "card_share",
            "zone",
            "Door",
            "PersonnelType",
        ]
        return df[feature_cols]

    X_new = engineer_single(temp_df)

    # 4) Predict with the pipeline:
    preds = pipeline.predict(X_new)   # +1 (inlier) or -1 (outlier)

    # 5) Build response: attach prediction back to original records
    response = []
    for i, rec in enumerate(records):
        response.append({
            "EmployeeID": rec.EmployeeID,
            "LocaleMessageTime": rec.LocaleMessageTime.isoformat(),
            "anomaly": int(preds[i]),  # +1 or -1
        })
    return response


# -------------- (Optional) Health-check on root --------------
@app.get("/", tags=["Root"])
def read_root():
    return {"message": "AI Employee Security API is running."}








# src/features.py

import pandas as pd

def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Mirror exactly the same transformations used in train.py.
    """
    df = df.copy()
    df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    df = df.dropna(subset=["LocaleMessageTime"])

    # Time‐based features
    df["hour"] = df["LocaleMessageTime"].dt.hour
    df["day_of_week"] = df["LocaleMessageTime"].dt.dayofweek
    df["is_weekend"] = df["day_of_week"].isin([5,6]).astype(int)

    # Boolean scenario columns should already exist from labelers
    for col in ["short_stay","long_stay","one_swipe","two_swipe","exit_first","simultaneous","anomaly","card_share"]:
        if col not in df.columns:
            df[col] = 0  # fill if missing
        else:
            df[col] = df[col].astype(int)

    # Categorical
    df["zone"] = df.get("zone", pd.Series(["Unknown"]*len(df))).fillna("Unknown")
    df["Door"] = df.get("Door", pd.Series(["Unknown"]*len(df))).fillna("Unknown")
    df["PersonnelType"] = df.get("PersonnelType", pd.Series(["Unknown"]*len(df))).fillna("Unknown")

    feature_cols = [
        "hour",
        "day_of_week",
        "is_weekend",
        "short_stay",
        "long_stay",
        "one_swipe",
        "two_swipe",
        "exit_first",
        "simultaneous",
        "anomaly",
        "card_share",
        "zone",
        "Door",
        "PersonnelType",
    ]
    return df[feature_cols]






cd C:\Users\W0024618\Desktop\ai-employee-security
python -m venv venv
venv\Scripts\activate
pip install --upgrade pip
pip install pandas scikit-learn joblib fastapi uvicorn python‐dotenv pyodbc sqlalchemy




cd src
python extract.py
# This loads data/access_logs.csv, parses dates, writes data/logs.pkl


cd src
python train.py















yes i want exact thsi type of  mopdel


Step 1: Define Use Cases (Scenarios) Like 

Start by deciding what insights or predictions you want the AI model to generate. Common scenarios include:
	•	Anomaly detection: Identify unusual access patterns.
	•	Employee movement pattern analysis.- As of now We are Working as a Security Analyst
Some times Employee Usee Twice of card 1 is self and another one is for his frend they mark their as well another employee attendace.

Also find Built Scenario using office time duration Whos Duration is very less , or Whos Duration is more.

Highglight those Employee Whos have only one Swipe for the day or only two Swipe for the day.
highlight those Employee Whos 1 st Swipe is Out.
Highlight those Employee pair wise whose swipe got exact same time as Well same door.
Highlight employee from their rejection detaiils trend.
	•	Unauthorized access prediction.

⸻

Step 2: Prepare and Label Data
	•	Use your SQL queries to fetch historical data (door access logs with timestamps, zones, employee info).
	•	Label your data if needed (e.g., normal, anomaly, after-hours, unauthorized).
	•	Clean data (handle missing timestamps, normalize employee IDs, ensure time format consistency).

yes


Extract meaningful features like:
	•	Time of entry/exit
	•	Duration of stay
	•	Zone visited
	•	Frequency of access
	•	Day of the week
	•	Entry direction (In/Out)

⸻

Step 4: Choose Modeling Approach
	•	Supervised Learning: If you have labeled data (e.g., unauthorized vs. authorized).
	•	Unsupervised Learning: For anomaly detection (clustering access patterns).
	•	Time-Series Models: For predicting future occupancy.
	•	Reinforcement Learning: For scenario optimization (less common initially).

⸻

Step 5: Model Training and Evaluation
	•	Use Python (with pandas, scikit-learn, XGBoost, or TensorFlow) to train models.
	•	Split data into training/validation/test sets.
	•	Evaluate using accuracy, precision, recall, or custom KPIs.

⸻

Step 6: Integrate AI with Backend
	•	Create a new route in your Node.js backend like /api/ai-insights.
	•	Call Python scripts or expose the model via a REST API using Flask/FastAPI.

⸻

Step 7: Monitor and Retrain
	•	Regularly re-train the model as new data flows in.
	•	Track model performance over time.



Steps i have done




C:\Users\W0024618\Desktop\ai-employee-security\src\api.py

# src/api.py

from fastapi import FastAPI
from myapp.data_controller import router as data_router

app = FastAPI(
    title="AI Employee Security Backend",
    description="Endpoints to fetch historical access-log data for AI training",
    version="1.0.0"
)

# Include the /raw-data endpoint
app.include_router(data_router, prefix="/api")


# Optionally, you can add a simple root health-check
@app.get("/")
async def root():
    return {"message": "AI Employee Security Backend is running"}


C:\Users\W0024618\Desktop\ai-employee-security\src\extract.py

import pandas as pd
import os
import re

# Project paths
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
DATA_DIR     = os.path.join(PROJECT_ROOT, "data")
CSV_NAME     = "access_logs.csv"
PICKLE_NAME  = "logs.pkl"

def detect_timestamp_column(path: str) -> str:
    """
    Inspect the CSV header columns and pick the first one containing 'time'.
    Strips any stray quotes or whitespace.
    """
    df_head = pd.read_csv(path, nrows=0)
    for col in df_head.columns:
        clean = col.strip().strip('"').strip("'")
        if re.search(r"time", clean, re.IGNORECASE):
            return clean
    # fallback to first column
    return df_head.columns[0].strip().strip('"').strip("'")
def load_csv(filename: str = CSV_NAME) -> pd.DataFrame:
    csv_path = os.path.join(DATA_DIR, filename)
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"No such file: {csv_path}")

    # Detect and clean timestamp column name
    raw_ts_col = detect_timestamp_column(csv_path)
    ts_col = raw_ts_col.strip().strip('"').strip("'")
    print(f"⏱️  Detected timestamp column: '{ts_col}'")

    # Load CSV without parse_dates
    df = pd.read_csv(csv_path)

    # Rename timestamp column
    df = df.rename(columns={raw_ts_col: "LocaleMessageTime"})

    # Force datetime conversion
    df["LocaleMessageTime"] = pd.to_datetime(
        df["LocaleMessageTime"], errors="coerce"
    )

    # Drop rows where datetime failed to parse
    df = df.dropna(subset=["LocaleMessageTime"])

    # Extract date and time
    df["Dateonly"]   = df["LocaleMessageTime"].dt.date
    df["Swipe_Time"] = df["LocaleMessageTime"].dt.time

    return df


def save_pickle(df: pd.DataFrame, filename: str = PICKLE_NAME):
    path = os.path.join(DATA_DIR, filename)
    df.to_pickle(path)
    print(f"✅ Saved DataFrame with {len(df)} rows to {path}")

if __name__ == "__main__":
    df = load_csv()
    print("✅ Rows loaded:", len(df))
    print(df.head())
    save_pickle(df)





C:\Users\W0024618\Desktop\ai-employee-security\src\labelers.py

# src/labelers.py

import pandas as pd
from datetime import timedelta

def label_anomaly(df: pd.DataFrame, window_minutes: int = 10, max_swipes: int = 5) -> pd.DataFrame:
    """
    Label 'anomaly' if an employee has more than `max_swipes` within `window_minutes`.
    """
    df = df.sort_values("LocaleMessageTime").copy()
    # Count swipes in rolling window per employee
    df["swipe_count_rolling"] = (
        df.groupby("EmployeeID")["LocaleMessageTime"]
          .rolling(f"{window_minutes}min")
          .count()
          .reset_index(0, drop=True)
    )
    df["anomaly"] = df["swipe_count_rolling"] > max_swipes
    return df

def label_card_share(df: pd.DataFrame, window: timedelta = timedelta(minutes=5)) -> pd.DataFrame:
    """
    Label 'card_share' when two different EmployeeIDs use the same CardNumber within `window`.
    """
    df = df.sort_values("LocaleMessageTime").copy()
    df["prev_employee"] = df.groupby("CardNumber")["EmployeeID"].shift(1)
    df["prev_time"]     = df.groupby("CardNumber")["LocaleMessageTime"].shift(1)
    df["card_share"] = (
        (df["EmployeeID"] != df["prev_employee"]) &
        (df["LocaleMessageTime"] - df["prev_time"] <= window)
    )
    return df

def label_duration(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute duration per day and label 'short_stay' (<1h) and 'long_stay' (>10h).
    """
    # Aggregate per EmployeeID & Dateonly
    agg = (
        df.groupby(["EmployeeID", "Dateonly"])["LocaleMessageTime"]
          .agg(["min", "max", "count"])
          .rename(columns={"min": "first_swipe", "max": "last_swipe", "count": "swipe_count"})
          .reset_index()
    )
    agg["duration_hours"] = (agg["last_swipe"] - agg["first_swipe"]).dt.total_seconds() / 3600
    agg["short_stay"] = agg["duration_hours"] < 1
    agg["long_stay"]  = agg["duration_hours"] > 10
    return agg

def label_low_swipe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Label 'one_swipe' or 'two_swipe' if total swipes per day ≤ 2.
    """
    counts = (
        df.groupby(["EmployeeID", "Dateonly"])
          .size()
          .reset_index(name="swipe_count")
    )
    counts["one_swipe"] = counts["swipe_count"] == 1
    counts["two_swipe"] = counts["swipe_count"] == 2
    return counts

def label_exit_first(df: pd.DataFrame) -> pd.DataFrame:
    """
    Label 'exit_first' if the first swipe of the day is OutDirection.
    """
    first = (
        df.sort_values("LocaleMessageTime")
          .groupby(["EmployeeID", "Dateonly"])
          .first()
          .reset_index()
    )
    first["exit_first"] = first["Direction"] == "OutDirection"
    return first

def label_simultaneous(df: pd.DataFrame) -> pd.DataFrame:
    """
    Label 'simultaneous' when two different EmployeeIDs swipe same Door at same time.
    """
    dup = (
        df.groupby(["Door", "LocaleMessageTime"])["EmployeeID"]
          .nunique()
          .reset_index(name="user_count")
    )
    dup["simultaneous"] = dup["user_count"] > 1
    # Merge back
    return df.merge(dup[["Door", "LocaleMessageTime", "simultaneous"]], 
                    on=["Door", "LocaleMessageTime"], how="left")

# TODO: Implement the remaining label functions:
# 7. label_rejection_trend()
# 8. label_unauthorized_access()
# 9. label_after_hours()
# 10. label_rapid_hop()
# 11. label_loiter_after_hours()
# 12. label_weekend_access()
# 13. label_holiday_access()
# 14. label_no_exit()
# 15. label_rapid_in_out()
# 16. label_zone_pingpong()
# 17. label_new_door()
# 18. label_high_volume()
# 19. label_time_window_violation()
# 20. label_failed_access_trend()





Now Help to next


